{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Introduction\n",
    "Memory Networks (MN) is introduced by Jason Weston et al. in 2014 to solve question-answering tasks [1]. MN is a neural network architecture which is accepting several pieces of evidence (sentences) and questions, answering them after finishing updating memories. They applied fixed hops to read evidences and updating memories and hard attention (chosing one explicit sentence) to update memories and eventually generate answers. In these project, I propose a new mechanism with hard/soft attentions and stop criteria based on reinforcement learning approaches (DQN). Evolutionary function approximation and several learning methods will be tested under the structure of MN. Facebook's bAbI dataset will be used to evaluate the new mechanism.\n",
    "\n",
    "The structure of this report is as follows.\n",
    "\n",
    "Section 2 reviews related work on Memory Networks and Deep Q-learning. Section 3 introduces my new Memory Networks mechanism combining DQN with End2End Memory Networks. Section 4 evaluates the new model by using Facebook's bAbi dataset. Section 5 discusses the future work.\n",
    "\n",
    "# 2 Related Work\n",
    "Question answering (QA) is a complex natural language processing task which requires a deep understanding of a text and the ability to reason over relevant facts.For example, given following factes,\n",
    "> _John picked up the apple._\n",
    "\n",
    "> _John went to the office._\n",
    "\n",
    "> _John went to the kitchen._\n",
    "\n",
    "> _John dropped the apple._\n",
    "\n",
    "> _Mary travelled to the office._\n",
    "\n",
    "The question is _Where was the apple before the kitchen?_ . In order to answer this question, firstly the fact _John picked up the apple._ is picked up, and then _John went to the kitchen._ & _John went to the office._ are selected to find the correct location of the apple. From this case, the basic reasonning process can be briefly described as follows.\n",
    "1. Read a question!\n",
    "2. Read facts.\n",
    "3. Have new knowledge.\n",
    "4. Back to (2) or answer the question\n",
    "-------------------------------------------\n",
    "\n",
    "## 2.1 Memory Networks\n",
    "Likewise, the End2End Memory Networks has the same procedure and can be described by 4 components[2]:\n",
    "\n",
    "1. **Read a question**, the input feature map $ I $ is used to converts the question into a input feature representation $ u $.\n",
    "2. **Read facts**, the input memory representation takes the inner product of $ u $ and each memory $ m_i $ followed by a softmax:\n",
    "$$\n",
    "p_i = Softmax(u^T m_i) \n",
    "$$\n",
    "3. **Have and new knowledge**, the output memory representation sums over the transformed inputs $ c_i $ weighted by the probability vector $ p_i $,\n",
    "$$\n",
    "o = \\sum_i {p_i c_i}\n",
    "$$\n",
    "4. **Answer the question**, the final prediction is from the the sum of the output vector $ o $ and the input embedding $ u $ through a weight matrix $ W $ and a softmax:\n",
    "$$\n",
    "\\hat{a} = Softmax(W(o + u)) \n",
    "$$\n",
    "![alt text](memn2n.png \"Strucutre of MemN2N\")\n",
    "However, there are two issues need to be considered. One of them is when stop updating $ u $ to predict answers (how many hops are needed).Intuitively, to use reinforcement learning to decide the next action (continue reading&updatig or stop to predict answers) enable the model to find the best choices. The other issue is how to balance exploitation and exploration if there are a large set of facts (can not explicitly have $ A $) at the time to read information from facts. Until now, this project has only finished testing the first idea, and the second one will be done in the near future.\n",
    "\n",
    "## 2.2 DQN\n",
    "Deep Q-learning (DQN) is the q-learning with a deep neural networks as its q-value estimation function. The loss function is shown as follows.\n",
    "$$\n",
    "L = [r + \\gamma \\max_{a'}Q(S',a') - Q(s,a)]^2\n",
    "$$\n",
    "Gieven a transition $(s,a,r,s')$, DQN use the neural networks to estimate $Q(s,a)$.\n",
    "$$\n",
    "Q(s,a) = f(s, \\theta)\n",
    "$$\n",
    "\n",
    "# 3 QMemN2N\n",
    "In the section, I propose a new mechanism (QMemN2N) which applies DQN to control the reading action of MemN2N. The states of QMemN2N is $ u_1, u_2, ..., u_n $; the actions are 0 (continue) and 1 (stop); the rewards are 1.0 when answer is correct, -1.0 when answer is wrong and 0 when the process continues to next hop. Here is the basic algorithm,\n",
    "~~~\n",
    "Train MemN2N, different hops share the same weighted vector $ A $ for reading and $ C $ for writing;\n",
    "Initialize QMemN2N by copying input feature map $ I $, weight matrix $ W $,  $A$ and $C$\n",
    "Train DQN, 20 hops maximum\n",
    "~~~\n",
    "![alt text](qmem.png \"Procedure of QMemN2N\")\n",
    "\n",
    "## 3.1 Implementation\n",
    "The basic implementation of MemN2N is from [domluna's work](https://github.com/domluna/memn2n). I modified its model structure from different $C$ for different hops to unique $C$ for all hops. And then I build the QMemN2N shown in [Appendix](#Appendix)\n",
    "\n",
    "# 4 Experiments\n",
    "The dataset bAbi (with 20 different tasks) is used to test QMemN2N. Firstly, I run MemN2N on different numbers of hops from 1 to 20 repectively. And then the QMemN2N model is built on the MemN2N with 2 hops. \n",
    "In the task 9 (Simple Negation), QMemN2N performs better than all MemN2N models, and the result is shown here.\n",
    "![alt text](task9.png \"Results on Task 9\")\n",
    "In the task 15 (Basic Deduction), QMemN2N performs much better than the MemN2N models with 2 hops, and the result is shown here.\n",
    "![alt text](task15.png \"Results on Task 15\")\n",
    "The next table shows the results on all tasks. The results of MemN2N is based on 2 hops, while the results of QMemN2N is based on dynamic hops decided by DQN.\n",
    "![alt text](result.png \"Results on All Tasks\")\n",
    "From this table, not all QMemN2N are better than MemN2N (QMemN2N works better on 11 of them), but still this is an encouraging results.\n",
    "\n",
    "# 5 Future Work\n",
    "In this project, I tested my first idea to dynamically stop to predict the answers. However, considering the data in bAbi is well separated into 20 categories, more experiments with mixed questions are needed. Also, right now I only trained the q-value estimator on current $u$, so more powerful estimators on multiple previous $u_{i-k}, ..., u_k$ and the difference between $u_k$ and $u_0$ are worth trying. Beside, to balance exploitation and exploration is more challenging but is with more potential.\n",
    "\n",
    "# Appendix\n",
    "The complete code are in file _QMemN2N.py_. Here, I only show the core code.\n",
    "~~~python\n",
    "model = MemN2N(batch_size, vocab_size, sentence_size, memory_size, FLAGS.embedding_size, session=sess,\n",
    "               hops=FLAGS.hops, max_grad_norm=FLAGS.max_grad_norm)\n",
    "for t in range(1, FLAGS.epochs+1):\n",
    "    # Stepped learning rate\n",
    "    if t - 1 <= FLAGS.anneal_stop_epoch:\n",
    "        anneal = 2.0 ** ((t - 1) // FLAGS.anneal_rate)\n",
    "    else:\n",
    "        anneal = 2.0 ** (FLAGS.anneal_stop_epoch // FLAGS.anneal_rate)\n",
    "    lr = FLAGS.learning_rate / anneal\n",
    "    #Train MemN2N\n",
    "    np.random.shuffle(batches)\n",
    "    total_cost = 0.0\n",
    "    for start, end in batches:\n",
    "        s = trainS[start:end]\n",
    "        q = trainQ[start:end]\n",
    "        a = trainA[start:end]\n",
    "        cost_t = model.batch_fit(s, q, a, lr)\n",
    "        total_cost += cost_t\n",
    "    if t % FLAGS.evaluation_interval == 0:\n",
    "        train_preds = []\n",
    "        for start in range(0, n_train, batch_size):\n",
    "            end = start + batch_size\n",
    "            s = trainS[start:end]\n",
    "            q = trainQ[start:end]\n",
    "            pred = model.predict(s, q)\n",
    "            train_preds += list(pred)\n",
    "\n",
    "        val_preds = model.predict(valS, valQ)\n",
    "        train_acc = metrics.accuracy_score(np.array(train_preds), train_labels)\n",
    "        val_acc = metrics.accuracy_score(val_preds, val_labels)\n",
    "\n",
    "        print('-----------------------')\n",
    "        print('Epoch', t)\n",
    "        print('Total Cost:', total_cost)\n",
    "        print('Training Accuracy:', train_acc)\n",
    "        print('Validation Accuracy:', val_acc)\n",
    "        print('-----------------------')\n",
    "#prediction on MemN2N\n",
    "test_preds = model.predict(testS, testQ)\n",
    "test_acc = metrics.accuracy_score(test_preds, test_labels)\n",
    "print(\"Testing Accuracy:\", test_acc)\n",
    "#initialize QMemN2N\n",
    "q_model = QMemN2N(model, 0.8)\n",
    "#train QMemN2N\n",
    "for t in range(1, FLAGS.epochs+1):\n",
    "    for i in range(n_train):\n",
    "        u0 = q_model.getFirstState(trainQ[i])\n",
    "        #get u1\n",
    "        r,u1 = q_model.takeAction(0, trainS[i], trainA[i], u0)\n",
    "        G = r\n",
    "        for j in range(20):\n",
    "            a = q_model.qvalue().argmax()\n",
    "            if np.random.random() < epsilon:\n",
    "                a = np.random.choice(range(2))\n",
    "            r,u2 = q_model.takeAction(a, trainS[i], trainA[i], u1)\n",
    "            G += r\n",
    "            q_model.updateQfunction(r, u1, a)\n",
    "            u1 = u2 #new states\n",
    "            if a == 1:\n",
    "                break\n",
    "~~~\n",
    "\n",
    "# Reference\n",
    "\n",
    "[1] Weston, Jason, Sumit Chopra, and Antoine Bordes. \"Memory networks.\" arXiv preprint arXiv:1410.3916 (2014).\n",
    "\n",
    "[2] Sukhbaatar, Sainbayar, Jason Weston, and Rob Fergus. \"End-to-end memory networks.\" Advances in neural information processing systems. 2015.\n",
    "\n",
    "[3] Mnih, Volodymyr, et al. \"Playing atari with deep reinforcement learning.\" arXiv preprint arXiv:1312.5602 (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
